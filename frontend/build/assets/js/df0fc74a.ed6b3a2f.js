"use strict";(globalThis.webpackChunkai_native_textbook=globalThis.webpackChunkai_native_textbook||[]).push([[988],{5722:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vision-language-action-models","title":"Module 4: Vision-Language-Action Models","description":"AI models integrating vision, language, and robotic action","source":"@site/docs/vision-language-action-models.md","sourceDirName":".","slug":"/vision-language-action-models","permalink":"/docs/vision-language-action-models","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"chapter_id":"ch04-vision-language-action-models"},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action Models","permalink":"/docs/modules/vla/intro"},"next":{"title":"Weekly Learning Plan for Physical AI & Humanoid Robotics","permalink":"/docs/weekly-plan"}}');var r=i(4848),l=i(8453);const o={sidebar_position:5,chapter_id:"ch04-vision-language-action-models"},t="Module 4: Vision-Language-Action Models",a={},c=[{value:"Introduction to Vision-Language-Action (VLA) Models",id:"introduction-to-vision-language-action-vla-models",level:2},{value:"What are VLA Models?",id:"what-are-vla-models",level:3},{value:"The VLA Architecture",id:"the-vla-architecture",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:4},{value:"End-to-End Learning",id:"end-to-end-learning",level:4},{value:"Key VLA Models",id:"key-vla-models",level:3},{value:"RT-1 (Robotics Transformer 1)",id:"rt-1-robotics-transformer-1",level:4},{value:"BC-Z (Behavior Cloning with Z-scale)",id:"bc-z-behavior-cloning-with-z-scale",level:4},{value:"VoxPoser",id:"voxposer",level:4},{value:"Training VLA Models",id:"training-vla-models",level:3},{value:"Data Requirements",id:"data-requirements",level:4},{value:"Data Collection Strategies",id:"data-collection-strategies",level:4},{value:"Loss Functions",id:"loss-functions",level:4},{value:"VLA in Embodied Intelligence",id:"vla-in-embodied-intelligence",level:3},{value:"Closed-Loop Control",id:"closed-loop-control",level:4},{value:"Task Generalization",id:"task-generalization",level:4},{value:"Implementation Challenges",id:"implementation-challenges",level:3},{value:"Visual Grounding",id:"visual-grounding",level:4},{value:"Action Space Mapping",id:"action-space-mapping",level:4},{value:"Real-Time Performance",id:"real-time-performance",level:4},{value:"Integration with Robot Systems",id:"integration-with-robot-systems",level:3},{value:"Control Architecture",id:"control-architecture",level:4},{value:"ROS 2 Integration",id:"ros-2-integration",level:4},{value:"VLA and Human-Robot Interaction",id:"vla-and-human-robot-interaction",level:3},{value:"Natural Language Interface",id:"natural-language-interface",level:4},{value:"Safety Considerations",id:"safety-considerations",level:4},{value:"Hardware Considerations for VLA",id:"hardware-considerations-for-vla",level:3},{value:"Compute Requirements",id:"compute-requirements",level:4},{value:"Sensor Requirements",id:"sensor-requirements",level:4},{value:"Future of VLA Models",id:"future-of-vla-models",level:3},{value:"Emerging Trends",id:"emerging-trends",level:4},{value:"Research Directions",id:"research-directions",level:4},{value:"Best Practices for VLA Implementation",id:"best-practices-for-vla-implementation",level:3},{value:"Try With AI",id:"try-with-ai",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-4-vision-language-action-models",children:"Module 4: Vision-Language-Action Models"})}),"\n",(0,r.jsxs)("div",{style:{textAlign:"center",margin:"2rem 0"},children:[(0,r.jsx)("img",{src:"https://images.unsplash.com/photo-1563207153-f403bf289096?w=1200&q=80",alt:"Vision-Language-Action Models",style:{width:"100%",maxWidth:"800px",borderRadius:"12px",boxShadow:"0 8px 16px rgba(0,0,0,0.1)"}}),(0,r.jsx)("p",{style:{fontSize:"0.9rem",color:"#666",marginTop:"0.5rem"},children:"AI models integrating vision, language, and robotic action"})]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-vision-language-action-vla-models",children:"Introduction to Vision-Language-Action (VLA) Models"}),"\n",(0,r.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent a paradigm shift in robotics, enabling robots to understand natural language instructions, perceive their environment, and execute complex tasks. These models bridge the gap between high-level human communication and low-level robotic control, forming a crucial component of embodied intelligence."}),"\n",(0,r.jsx)(n.h3,{id:"what-are-vla-models",children:"What are VLA Models?"}),"\n",(0,r.jsx)(n.p,{children:"VLA models are neural architectures that integrate three modalities:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Processing visual information from cameras and sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language"}),": Understanding natural language commands and descriptions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": Generating motor commands to control robotic systems"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'This integration allows robots to perform tasks like "Pick up the red cup on the left side of the table" by processing the language instruction, identifying the relevant objects in the visual scene, and executing the appropriate motor actions.'}),"\n",(0,r.jsx)(n.h3,{id:"the-vla-architecture",children:"The VLA Architecture"}),"\n",(0,r.jsxs)("div",{style:{textAlign:"center",margin:"2rem 0"},children:[(0,r.jsx)("img",{src:"https://images.unsplash.com/photo-1677442136019-21780ecad995?w=800&q=80",alt:"Multimodal AI Integration",style:{width:"100%",maxWidth:"600px",borderRadius:"8px",boxShadow:"0 4px 8px rgba(0,0,0,0.1)"}}),(0,r.jsx)("p",{style:{fontSize:"0.9rem",color:"#666",marginTop:"0.5rem"},children:"Fusion of vision, language, and action in AI models"})]}),"\n",(0,r.jsx)(n.h4,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,r.jsx)(n.p,{children:"VLA models typically use transformer architectures to fuse information across modalities:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass VLAModel(nn.Module):\r\n    def __init__(self, vision_encoder, language_encoder, action_head):\r\n        super().__init__()\r\n        self.vision_encoder = vision_encoder\r\n        self.language_encoder = language_encoder\r\n        self.fusion_layer = nn.TransformerEncoder(\r\n            nn.TransformerEncoderLayer(d_model=512, nhead=8),\r\n            num_layers=6\r\n        )\r\n        self.action_head = action_head\r\n\r\n    def forward(self, images, language_commands):\r\n        # Encode visual and language inputs\r\n        vision_features = self.vision_encoder(images)\r\n        lang_features = self.language_encoder(language_commands)\r\n\r\n        # Fuse modalities\r\n        fused_features = self.fusion_layer(\r\n            torch.cat([vision_features, lang_features], dim=1)\r\n        )\r\n\r\n        # Generate actions\r\n        actions = self.action_head(fused_features)\r\n        return actions\n"})}),"\n",(0,r.jsx)(n.h4,{id:"end-to-end-learning",children:"End-to-End Learning"}),"\n",(0,r.jsx)(n.p,{children:"VLA models are typically trained end-to-end on large datasets of:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Robot demonstrations"}),"\n",(0,r.jsx)(n.li,{children:"Language annotations"}),"\n",(0,r.jsx)(n.li,{children:"Visual observations"}),"\n",(0,r.jsx)(n.li,{children:"Action sequences"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-vla-models",children:"Key VLA Models"}),"\n",(0,r.jsx)(n.h4,{id:"rt-1-robotics-transformer-1",children:"RT-1 (Robotics Transformer 1)"}),"\n",(0,r.jsx)(n.p,{children:"Developed by Google, RT-1 is a transformer-based model that:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Processes images and natural language commands"}),"\n",(0,r.jsx)(n.li,{children:"Outputs motor actions for robot control"}),"\n",(0,r.jsx)(n.li,{children:"Demonstrates strong generalization to new tasks"}),"\n",(0,r.jsx)(n.li,{children:"Uses a large pre-trained vision-language model foundation"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"bc-z-behavior-cloning-with-z-scale",children:"BC-Z (Behavior Cloning with Z-scale)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Focuses on human demonstration learning"}),"\n",(0,r.jsx)(n.li,{children:"Incorporates human intention understanding"}),"\n",(0,r.jsx)(n.li,{children:"Uses hierarchical action representations"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"voxposer",children:"VoxPoser"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Combines vision-language models with spatial reasoning"}),"\n",(0,r.jsx)(n.li,{children:"Generates 6-DOF poses for manipulation tasks"}),"\n",(0,r.jsx)(n.li,{children:"Enables complex spatial reasoning from language"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-vla-models",children:"Training VLA Models"}),"\n",(0,r.jsx)(n.h4,{id:"data-requirements",children:"Data Requirements"}),"\n",(0,r.jsx)(n.p,{children:"VLA models require diverse training data:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robot Demonstrations"}),": Task execution with various robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Task Coverage"}),": Diverse set of tasks and environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language Variations"}),": Multiple ways to describe the same task"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual Diversity"}),": Different lighting, objects, and environments"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"data-collection-strategies",children:"Data Collection Strategies"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human Demonstrations"}),": Humans teleoperating robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autonomous Data Collection"}),": Robots collecting their own data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Simulation-to-Real Transfer"}),": Using simulation data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Robot Datasets"}),": Data from various robot platforms"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"loss-functions",children:"Loss Functions"}),"\n",(0,r.jsx)(n.p,{children:"VLA training typically uses:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Behavior Cloning Loss"}),": Matching demonstrated actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language Consistency Loss"}),": Ensuring language understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual Grounding Loss"}),": Aligning visual perception with actions"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"vla-in-embodied-intelligence",children:"VLA in Embodied Intelligence"}),"\n",(0,r.jsx)(n.h4,{id:"closed-loop-control",children:"Closed-Loop Control"}),"\n",(0,r.jsx)(n.p,{children:"VLA models enable closed-loop control where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The robot continuously perceives its environment"}),"\n",(0,r.jsx)(n.li,{children:"Understands high-level goals through language"}),"\n",(0,r.jsx)(n.li,{children:"Executes appropriate actions"}),"\n",(0,r.jsx)(n.li,{children:"Adapts based on feedback"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"task-generalization",children:"Task Generalization"}),"\n",(0,r.jsx)(n.p,{children:"VLA models demonstrate remarkable generalization:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Performing new tasks without explicit programming"}),"\n",(0,r.jsx)(n.li,{children:"Handling novel object arrangements"}),"\n",(0,r.jsx)(n.li,{children:"Adapting to different environments"}),"\n",(0,r.jsx)(n.li,{children:"Following complex multi-step instructions"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"implementation-challenges",children:"Implementation Challenges"}),"\n",(0,r.jsx)(n.h4,{id:"visual-grounding",children:"Visual Grounding"}),"\n",(0,r.jsx)(n.p,{children:"Accurately identifying objects mentioned in language:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"The cup near the laptop" vs "the cup on the table"'}),"\n",(0,r.jsx)(n.li,{children:"Handling ambiguous references"}),"\n",(0,r.jsx)(n.li,{children:"Dealing with partially observed scenes"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"action-space-mapping",children:"Action Space Mapping"}),"\n",(0,r.jsx)(n.p,{children:"Converting high-level language to low-level actions:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Continuous vs discrete action spaces"}),"\n",(0,r.jsx)(n.li,{children:"Coordinate system transformations"}),"\n",(0,r.jsx)(n.li,{children:"Multi-step task decomposition"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"real-time-performance",children:"Real-Time Performance"}),"\n",(0,r.jsx)(n.p,{children:"Meeting real-time constraints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Efficient inference on robot hardware"}),"\n",(0,r.jsx)(n.li,{children:"Latency considerations for safety"}),"\n",(0,r.jsx)(n.li,{children:"Resource optimization"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"integration-with-robot-systems",children:"Integration with Robot Systems"}),"\n",(0,r.jsx)(n.h4,{id:"control-architecture",children:"Control Architecture"}),"\n",(0,r.jsx)(n.p,{children:"VLA models typically integrate with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low-level Controllers"}),": Joint position/velocity controllers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Motion Planning"}),": Trajectory generation and obstacle avoidance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception Stack"}),": Object detection and scene understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task Planning"}),": High-level task decomposition"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass VLANode(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_node')\r\n\r\n        # Initialize VLA model\r\n        self.vla_model = self.load_vla_model()\r\n\r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/image_raw', self.image_callback, 10\r\n        )\r\n        self.command_sub = self.create_subscription(\r\n            String, '/command', self.command_callback, 10\r\n        )\r\n\r\n        # Publisher\r\n        self.action_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        self.current_image = None\r\n        self.current_command = None\r\n\r\n    def image_callback(self, msg):\r\n        self.current_image = self.process_image(msg)\r\n\r\n    def command_callback(self, msg):\r\n        self.current_command = msg.data\r\n        if self.current_image is not None:\r\n            action = self.vla_model(\r\n                self.current_image,\r\n                self.current_command\r\n            )\r\n            self.publish_action(action)\r\n\r\n    def publish_action(self, action):\r\n        cmd_msg = Twist()\r\n        cmd_msg.linear.x = action[0]\r\n        cmd_msg.angular.z = action[1]\r\n        self.action_pub.publish(cmd_msg)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"vla-and-human-robot-interaction",children:"VLA and Human-Robot Interaction"}),"\n",(0,r.jsx)(n.h4,{id:"natural-language-interface",children:"Natural Language Interface"}),"\n",(0,r.jsx)(n.p,{children:"VLA models enable natural human-robot interaction:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Conversational commands"}),"\n",(0,r.jsx)(n.li,{children:"Clarification requests"}),"\n",(0,r.jsx)(n.li,{children:"Feedback incorporation"}),"\n",(0,r.jsx)(n.li,{children:"Learning from corrections"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Action validation and safety checks"}),"\n",(0,r.jsx)(n.li,{children:"Human-in-the-loop supervision"}),"\n",(0,r.jsx)(n.li,{children:"Fail-safe mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Uncertainty quantification"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"hardware-considerations-for-vla",children:"Hardware Considerations for VLA"}),"\n",(0,r.jsx)(n.h4,{id:"compute-requirements",children:"Compute Requirements"}),"\n",(0,r.jsx)(n.p,{children:"VLA models typically require:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPUs"}),": For real-time inference (Jetson AGX Orin, RTX series)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory"}),": Large models require significant RAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Power"}),": Consider power constraints for mobile robots"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"sensor-requirements",children:"Sensor Requirements"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cameras"}),": High-resolution RGB cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Sensors"}),": For 3D understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Microphones"}),": For voice commands (optional)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"future-of-vla-models",children:"Future of VLA Models"}),"\n",(0,r.jsx)(n.h4,{id:"emerging-trends",children:"Emerging Trends"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal Pretraining"}),": Larger pre-trained models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Few-Shot Learning"}),": Learning new tasks from few examples"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interactive Learning"}),": Learning through human interaction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embodied GPT Models"}),": Large language models for embodied tasks"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"research-directions",children:"Research Directions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Long-Horizon Tasks"}),": Multi-step task execution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Social Interaction"}),": Understanding human social cues"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tool Use"}),": Complex tool manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Collaborative Robots"}),": Multi-robot coordination"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"best-practices-for-vla-implementation",children:"Best Practices for VLA Implementation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Start Simple"}),": Begin with basic tasks before complex ones"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Quality"}),": Prioritize high-quality demonstration data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety First"}),": Implement comprehensive safety checks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluation"}),": Use diverse evaluation metrics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Iterative Development"}),": Continuously improve based on performance"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"try-with-ai",children:"Try With AI"}),"\n",(0,r.jsx)(n.p,{children:"Try asking your AI companion about implementing a specific VLA model on your robot platform, or ask for guidance on collecting training data for VLA models. You can also inquire about the differences between various VLA architectures and their trade-offs for different applications."})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const r={},l=s.createContext(r);function o(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);