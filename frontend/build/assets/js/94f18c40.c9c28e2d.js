"use strict";(globalThis.webpackChunkai_native_textbook=globalThis.webpackChunkai_native_textbook||[]).push([[100],{4099:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>r});const t=JSON.parse('{"id":"modules/vla/intro","title":"Vision-Language-Action Models","description":"This module covers Vision-Language-Action models for Physical AI and Humanoid Robotics applications.","source":"@site/docs/modules/vla/intro.md","sourceDirName":"modules/vla","slug":"/modules/vla/intro","permalink":"/docs/modules/vla/intro","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Vision-Language-Action Models"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: NVIDIA Isaac Ecosystem - Perception, Navigation, and Manipulation","permalink":"/docs/nvidia-isaac-ecosystem"},"next":{"title":"Module 4: Vision-Language-Action Models","permalink":"/docs/vision-language-action-models"}}');var a=i(4848),o=i(8453);const s={title:"Vision-Language-Action Models"},l="Vision-Language-Action (VLA) Models for Robotics",c={},r=[{value:"VLA Fundamentals",id:"vla-fundamentals",level:2},{value:"Practical Applications",id:"practical-applications",level:2},{value:"Interactive Learning",id:"interactive-learning",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"vision-language-action-vla-models-for-robotics",children:"Vision-Language-Action (VLA) Models for Robotics"})}),"\n",(0,a.jsx)(e.p,{children:"This module covers Vision-Language-Action models for Physical AI and Humanoid Robotics applications."}),"\n",(0,a.jsx)(e.h2,{id:"vla-fundamentals",children:"VLA Fundamentals"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding multimodal AI"}),"\n",(0,a.jsx)(e.li,{children:"Vision processing and interpretation"}),"\n",(0,a.jsx)(e.li,{children:"Language understanding for robotics"}),"\n",(0,a.jsx)(e.li,{children:"Action generation and execution"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"practical-applications",children:"Practical Applications"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Robot manipulation tasks"}),"\n",(0,a.jsx)(e.li,{children:"Human-robot interaction"}),"\n",(0,a.jsx)(e.li,{children:"Task planning and execution"}),"\n",(0,a.jsx)(e.li,{children:"Real-world deployment scenarios"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"interactive-learning",children:"Interactive Learning"}),"\n",(0,a.jsx)(e.p,{children:"Select any text in this chapter and ask the AI assistant for clarification or deeper explanations about VLA concepts."})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>l});var t=i(6540);const a={},o=t.createContext(a);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);